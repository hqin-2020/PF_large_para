{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzntH2XxWPND"
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "This notebook presents libraries and tools for making Python code run on\n",
    "NVIDIA GPUs!! The path to running on GPUs goes through improving CPU\n",
    "performance but the target is to run on GPUs.\n",
    "\n",
    "The notebook was tested on the Anaconda Python distribution for Python 3.x.\n",
    "This container should have all of the package dependencies.\n",
    "\n",
    "This notebook follows the accompanying slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL6venP1r8St"
   },
   "source": [
    "Let's check which CPU and GPU you're using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ey_CxMiGrBQ-",
    "outputId": "6b2dc780-0742-4480-8ce5-b6fef47bb7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbhzteCIq64z",
    "outputId": "3ff65663-c643-4139-e3cc-efd2668487e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              2\n",
      "On-line CPU(s) list: 0,1\n",
      "Thread(s) per core:  2\n",
      "Core(s) per socket:  1\n",
      "Socket(s):           1\n",
      "NUMA node(s):        1\n",
      "Vendor ID:           GenuineIntel\n",
      "CPU family:          6\n",
      "Model:               79\n",
      "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "Stepping:            0\n",
      "CPU MHz:             2199.998\n",
      "BogoMIPS:            4399.99\n",
      "Hypervisor vendor:   KVM\n",
      "Virtualization type: full\n",
      "L1d cache:           32K\n",
      "L1i cache:           32K\n",
      "L2 cache:            256K\n",
      "L3 cache:            56320K\n",
      "NUMA node0 CPU(s):   0,1\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNuNSb-4r9FW"
   },
   "source": [
    "We can write Python and execute it inside a code block: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgoOIA5psXlz",
    "outputId": "cad80377-fcf5-457f-db5a-4fba93bc7e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye world!!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Goodbye world!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofyXigMlWPNF"
   },
   "source": [
    "<a id='Simple Python Example with Numba'></a>\n",
    "# 1. Simple Python Example Using Numba - CPU and GPU\n",
    "\n",
    "\n",
    "This is a simple example that adds two vectors to create a third vector.\n",
    "The computational work is done in a function. To create enough computational\n",
    "work, the vector length is quite large (1,000,000,000).\n",
    "These are single precision vectors.\n",
    "\n",
    "The example starts with a simple Python example with no compiling.\n",
    "Just in case you are interested a simple timing of the addition is\n",
    "printed at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8vMu8ePWPNG"
   },
   "source": [
    "<a id='Simple Python Example'></a>\n",
    "## 1.1 Starting Point - Simple Python Code\n",
    "\n",
    "The code below is the baseline Python code (the starting point). It takes the tanh of each elemental along the diagonal of a matrix, and adds it to the matrix.\n",
    "The matrix is created using NumPy. The time it takes to perform the operation\n",
    "is measured using the \"timeit\" module in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlCGzQSeWPNG",
    "outputId": "42cbbc3f-aeef-46b0-8ec9-7450bf1138ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed in 0.201477 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "x = np.arange(100000000).reshape(10000, 10000)\n",
    "\n",
    "\n",
    "def trace_func(a): \n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   \n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace             \n",
    "\n",
    "start = timer()\n",
    "trace_func(x)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dweBH-4AWPNL"
   },
   "source": [
    "<a id='Simple Python Example - CPU'></a>\n",
    "## 1.2 Simple Python - @jit Examples\n",
    "\n",
    "The following examples present code that uses Numba @jit to compile the <tt>trace_func</tt> function in the code. Various options are presented for both CPU compiling\n",
    "and compiling for the NVIDIA GPU. These options use various decorators and\n",
    "options. The discussion to accompany these examples are in the slide deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjZgs6eBWPNM"
   },
   "source": [
    "<a id='Simple Python Example single core'></a>\n",
    "### 1.2.1 <tt>@jit</tt> with default target (single CPU core)\n",
    "\n",
    "This example uses the <tt>@jit</tt> decorator in Numba. By default this targets a\n",
    "single core on the CPU.\n",
    "\n",
    "You might try running the cell a few times to get an idea of the average\n",
    "wall clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RG1N795WPNN",
    "outputId": "8f42f495-cdf3-44d3-80f5-26b07622c9cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed in 0.152036 s\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "x = np.arange(100000000).reshape(10000, 10000)\n",
    "\n",
    "@jit\n",
    "def trace_func(a): \n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "trace_func(x)\n",
    "\n",
    "start = timer()\n",
    "trace_func(x)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3EmFYJyWPNQ"
   },
   "source": [
    "<a id='Simple Python Example multi-core'></a>\n",
    "### 1.2.2 <tt>@jit</tt> with multi-core target and no Python fallback\n",
    "\n",
    "This targets multiple CPU cores using the  <tt>parallel=True</tt>  option in  <tt>@jit</tt>.\n",
    "\n",
    "I also use the option  <tt>nopython=True</tt>  so I can catch my code\n",
    "mistakes. It is a default option, but I like to specify so I know exactly\n",
    "what options I'm using. This option disables any Python fall-back in case\n",
    "Numba cannot compile the code.\n",
    "\n",
    "You might try running the cell a few times to get an idea of the average\n",
    "wall clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APCxHw4NWPNR",
    "outputId": "010c9d14-a372-4f37-b322-1e1151b91388"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed in 0.209793 s\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "x = np.arange(100000000).reshape(10000, 10000)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def trace_func(a): \n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):  \n",
    "        trace += np.tanh(a[i, i]) \n",
    "    return a + trace              \n",
    "\n",
    "trace_func(x)\n",
    "\n",
    "start = timer()\n",
    "trace_func(x)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MdhyXVtWPNU"
   },
   "source": [
    "<strong>Note</strong>: Currently, we only measure the time for the compiled\n",
    "code. We don't measure\n",
    "the time that includes the compile time. Therefore, the elapsed wall clock\n",
    "times should not vary too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqsEkSOQWPNV"
   },
   "source": [
    "<a id='Simple Python Example defaults cache'></a>\n",
    "### 1.2.3 <tt>@jit</tt> with defaults, caching, and no Python fallback\n",
    "\n",
    "This example uses the defaults again, but it also caches the compiled code\n",
    "so it can be reused (<tt>cache=True</tt>). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udftOtNBWPNV"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "x = np.arange(100000000).reshape(10000, 10000)\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def trace_func(a): \n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):  \n",
    "        trace += np.tanh(a[i, i]) \n",
    "    return a + trace              \n",
    "\n",
    "trace_func(x)\n",
    "\n",
    "start = timer()\n",
    "trace_func(x)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abQ6ci6CWPNZ"
   },
   "source": [
    ":<a id='Simple Python Example multicore cache'></a>\n",
    "### 1.2.4 <tt>@jit</tt> with parallel, caching, and no Python fallback\n",
    "\n",
    "This example adds the option <tt>parallel=True</tt> (multi-core) to the\n",
    "previous options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q1sdM31WPNZ"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "x = np.arange(100000000).reshape(10000, 10000)\n",
    "\n",
    "@jit(nopython=True, parallel=True, cache=True)\n",
    "def trace_func(a): \n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):  \n",
    "        trace += np.tanh(a[i, i]) \n",
    "    return a + trace              \n",
    "\n",
    "trace_func(x)\n",
    "\n",
    "start = timer()\n",
    "trace_func(x)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ6rA-CAWPNd"
   },
   "source": [
    "<a id='Simple Python Example vectorize no type signature single core'></a>\n",
    "### 1.3 Numba <tt>@vectorize</tt> decorator \n",
    "\n",
    "\n",
    "Below is the Python code example for this exercise, without any decorator applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsE5VpGeg64m"
   },
   "source": [
    "### 1.3.1 Python Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MEDjB4IWPNe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def rel_diff(x, y): \n",
    "  return 2 * (x - y) / (x + y)\n",
    "\n",
    "a = np.arange(1000, dtype = np.float32)\n",
    "b = a * 2 + 1        \n",
    "\n",
    "start = timer()\n",
    "rel_diff(a, b)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMQU5s3DOSDM"
   },
   "source": [
    "This example uses the @vectorize decorator targeting the CPU (default target). \n",
    "Remember that the <tt>@vectorize</tt> decorator compiles functions that perform\n",
    "element-by-element operations (that is, the same operation to every element\n",
    "in the array). \n",
    "\n",
    "In general, you give the <tt>@vectorize</tt> decorator a type signature that\n",
    "tells Numba how to build the compiled code for a specific data type (you can\n",
    "specify more than one which is really cool). Here is an example,\n",
    "\n",
    "\n",
    "<codeblock>\n",
    "    @vectorize(['float32(float32, float32)'])\n",
    "</codeblock>\n",
    "\n",
    "\n",
    "The data type specification after the decorator is the \"type signature\".\n",
    "\n",
    "You don't have to specify a type signature. If you don't, then Numba will\n",
    "create a dynamic universal function (DUFunc). This dynamically compilers a\n",
    "new kernel when the function is called with a data type that wasn't previously\n",
    "used. That is, it will compile the function for every specific data type in\n",
    "your code. This can have an impact if your use multiple data types with the same\n",
    "function. You can think of this approach as \"call-time\" or \"lazy\" compilation.\n",
    "\n",
    "Below, see the example function with the addition of the @vectorize decorator. Note that no type signature is supplied in the function definition, so Numba will infer the types when the function is first called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv0aOKWZhS3V"
   },
   "source": [
    "### 1.3.2 <tt>@vectorize</tt> Without Type Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeCS_7-1OSvN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize\n",
    "def rel_diff(x, y): \n",
    "  return 2 * (x - y) / (x + y)\n",
    "\n",
    "a = np.arange(1000, dtype = np.float32)\n",
    "b = a * 2 + 1        \n",
    "\n",
    "rel_diff(a, b)\n",
    "\n",
    "start = timer()\n",
    "rel_diff(a, b)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hckMuli1OjEP"
   },
   "source": [
    "And finally the same function with the type signature defined. Since the signature is supplied with the function definition, Numba will know the type signature when the function definition is first encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nw0qgiHhh8u"
   },
   "source": [
    "### 1.3.3 <tt>@vectorize</tt> With Type Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97kED7FAOjd4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'])\n",
    "def rel_diff(x, y): \n",
    "  return 2 * (x - y) / (x + y)\n",
    "\n",
    "a = np.arange(1000, dtype = np.float32)\n",
    "b = a * 2 + 1        \n",
    "\n",
    "rel_diff(a, b)\n",
    "\n",
    "start = timer()\n",
    "rel_diff(a, b)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOlbFx9PWPNk"
   },
   "source": [
    "<a id='Simple Python Example vectorize type signature single core'></a>\n",
    "### 1.3.4 @vectorize decorator for CPUs with single CPU core target\n",
    "\n",
    "There are multiple hardware targets available with the @vectorize decorator. This is a trivial example, but it illustrates that the default target for the\n",
    "<tt>@vectorize</tt> decorator is a single core ( <tt>target='cpu'</tt> ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HBxZO78WPNl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cpu')\n",
    "def rel_diff(x, y): \n",
    "  return 2 * (x - y) / (x + y)\n",
    "\n",
    "a = np.arange(1000, dtype = np.float32)\n",
    "b = a * 2 + 1        \n",
    "\n",
    "rel_diff(a, b)\n",
    "\n",
    "start = timer()\n",
    "rel_diff(a, b)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrbGA2gTWPNo"
   },
   "source": [
    "<a id='Simple Python Example vectorize type signature parallel'></a>\n",
    "### 1.3.5 @vectorize decorator using all CPU cores\n",
    "\n",
    "This example changes the target to use all of the cores (multi-core). The target name\n",
    "is simple <tt>parallel</tt>. Even though all of the cores are being used, the performance\n",
    "may not improve. It takes time to move data around as needed across the cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSIR5ZJQWPNp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='parallel')\n",
    "def rel_diff(x, y): \n",
    "  return 2 * (x - y) / (x + y)\n",
    "\n",
    "a = np.arange(1000, dtype = np.float32)\n",
    "b = a * 2 + 1        \n",
    "\n",
    "rel_diff(a, b)\n",
    "\n",
    "start = timer()\n",
    "rel_diff(a, b)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMjUgR3oWPNw"
   },
   "source": [
    "<a id='Simple Python Example - GPU'></a>\n",
    "## 1.4 Simple Python - GPU Examples\n",
    "\n",
    "The examples below explore using the GPU as a target with the vectorize decorator\n",
    "and the <tt>cuda</tt> target, as well as using the  <tt>@cuda.jit</tt>  decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ZUuiznWPNx"
   },
   "source": [
    "<a id='Simple Python Example - Vectorize GPU'></a>\n",
    "### 1.4.1 <tt>@vectorize</tt> Decorator with a CUDA Target\n",
    "\n",
    "Porting Python code to the GPU can be very easy using the code from the  <tt>@vectorize</tt>\n",
    "decorator. For most code, all you need to do is change the *target* in the decorator to\n",
    "<tt>cuda</tt>.\n",
    "\n",
    "Notice how Numba takes care of copying the data to the GPU and copying it back. Numba\n",
    "also takes care of defining the arrays on the GPU.\n",
    "\n",
    "Try running the cell several times to get an idea of the compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa5A2YSgWPNy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def rel_diff(x, y): \n",
    "  return 2 * (x - y) / (x + y)\n",
    "\n",
    "a = np.arange(1000, dtype = np.float32)\n",
    "b = a * 2 + 1        \n",
    "\n",
    "rel_diff(a, b)\n",
    "\n",
    "start = timer()\n",
    "rel_diff(a, b)\n",
    "dt = timer() - start\n",
    "\n",
    "print(\"Computed in %f s\" % dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH75fqmaWPN1"
   },
   "source": [
    "<a id='Simple Python Example - cud.jit GPU'></a>\n",
    "### 1.4.2 <tt>@cuda.jit</tt> Decorator\n",
    "\n",
    "You can also use a different decorator,  <tt>@cuda.jit</tt>, that allows you to\n",
    "write Python code that is more CUDA-like. This offers\n",
    "you greater flexibility and control over your code and possibly better performance.\n",
    "However, you will need to know a fair amount about CUDA  before using it.\n",
    "The link below is a good place to get more information.\n",
    "\n",
    "\n",
    "http://numba.pydata.org/numba-doc/dev/cuda/index.html\n",
    "\n",
    "    \n",
    "As explained in the slides, using the  <tt>cuda.jit</tt>  decorator requires some extra coding.\n",
    "You have to explicitly write the loops in your code.\n",
    "Also, everything you pass into or out-of the compiled function, has to be a NumPy\n",
    "array (even scalars which are NumPy arrays of size 1). However, you can define simple\n",
    "scalars in the function that are not NumPy arrays. For example, loop counters.\n",
    "\n",
    "Pay close attention to how the data is passed to the function - it is much more\n",
    "like functions in C where everything all data is passed through the function call. \n",
    "\n",
    "To prepare for using <tt>@cuda.jit</tt>, let's introduce a new example. In this exercise, we will accelerate a Mandelbrot fractal computation using CUDA Python via Numba. Starting with the plain Python implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66GTieeVWPN1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import imshow, show\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def mandel(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters\n",
    "\n",
    "\n",
    "#The whole image loop...\n",
    "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n",
    "  height = image.shape[0]\n",
    "  width = image.shape[1]\n",
    "\n",
    "  pixel_size_x = (max_x - min_x) / width\n",
    "  pixel_size_y = (max_y - min_y) / height\n",
    "\n",
    "  for x in range(width):\n",
    "    real = min_x + x * pixel_size_x\n",
    "    for y in range(height):\n",
    "      imag = min_y + y * pixel_size_y\n",
    "      color = mandel(real, imag, iters)\n",
    "      image[y, x] = color\n",
    "\n",
    "      \n",
    "image = np.zeros((1024, 1536), dtype = np.uint8)\n",
    "start = timer()\n",
    "create_fractal(-2.0, 1.0, -1.0, 1.0, image, 20) \n",
    "dt = timer() - start\n",
    "\n",
    "print (\"Mandelbrot created in %f s\" % dt)\n",
    "imshow(image)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3fHnRBJRCr8"
   },
   "source": [
    "Now let's add @jit decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJSUMbVrRU5k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import imshow, show\n",
    "from timeit import default_timer as timer\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def mandel(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters\n",
    "\n",
    "\n",
    "#The whole image loop...\n",
    "@jit\n",
    "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n",
    "  height = image.shape[0]\n",
    "  width = image.shape[1]\n",
    "\n",
    "  pixel_size_x = (max_x - min_x) / width\n",
    "  pixel_size_y = (max_y - min_y) / height\n",
    "\n",
    "  for x in range(width):\n",
    "    real = min_x + x * pixel_size_x\n",
    "    for y in range(height):\n",
    "      imag = min_y + y * pixel_size_y\n",
    "      color = mandel(real, imag, iters)\n",
    "      image[y, x] = color\n",
    "\n",
    "      \n",
    "image = np.zeros((1024, 1536), dtype = np.uint8)\n",
    "start = timer()\n",
    "create_fractal(-2.0, 1.0, -1.0, 1.0, image, 20) \n",
    "dt = timer() - start\n",
    "\n",
    "print (\"Mandelbrot created in %f s\" % dt)\n",
    "imshow(image)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwMuFDTZRg4u"
   },
   "source": [
    "And now with @cuda.jit. Try changing the decorator and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRycRZtqSBp-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import imshow, show\n",
    "from timeit import default_timer as timer\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def mandel(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters\n",
    "\n",
    "\n",
    "#The whole image loop...\n",
    "@cuda.jit\n",
    "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n",
    "  height = image.shape[0]\n",
    "  width = image.shape[1]\n",
    "\n",
    "  pixel_size_x = (max_x - min_x) / width\n",
    "  pixel_size_y = (max_y - min_y) / height\n",
    "\n",
    "  for x in range(width):\n",
    "    real = min_x + x * pixel_size_x\n",
    "    for y in range(height):\n",
    "      imag = min_y + y * pixel_size_y\n",
    "      color = mandel(real, imag, iters)\n",
    "      image[y, x] = color\n",
    "\n",
    "      \n",
    "image = np.zeros((1024, 1536), dtype = np.uint8)\n",
    "start = timer()\n",
    "create_fractal(-2.0, 1.0, -1.0, 1.0, image, 20) \n",
    "dt = timer() - start\n",
    "\n",
    "print (\"Mandelbrot created in %f s\" % dt)\n",
    "imshow(image)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7WaMtbTUvez"
   },
   "source": [
    "First the caller. We have to define a configuration of threads to do our work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeXSbbPlVo7B"
   },
   "outputs": [],
   "source": [
    "image = np.zeros((1024, 1536), dtype = np.uint8)\n",
    "\n",
    "#Create grid of 32x32 blocks, one thread per pixel\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "nthreads = 32\n",
    "nblocksy = (height // nthreads) + 1 #33\n",
    "nblocksx = (width // nthreads) + 1 #49\n",
    "\n",
    "\n",
    "config = (nblocksx, nblocksy), (nthreads, nthreads)\n",
    "\n",
    "\n",
    "\n",
    "#start = timer()\n",
    "#create_fractal[config](-2.0, 1.0, -1.0, 1.0, image, 20) \n",
    "#dt = timer() - start\n",
    "\n",
    "#print (\"Mandelbrot created in %f s\" % dt)\n",
    "#imshow(image)\n",
    "#show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOeJHigcTaTZ"
   },
   "source": [
    "Next let's look at the whole-image loop, create_fractal. We will have a team of many GPU threads doing our work for us - one thread per pixel. So we can remove the loops that distribute multiple work items to a single thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdGhy1tYT4db"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba import *\n",
    "\n",
    "#mandel_gpu = cuda.jit(device=True)(mandel)\n",
    "\n",
    "@cuda.jit\n",
    "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n",
    "  height = image.shape[0]\n",
    "  width = image.shape[1]\n",
    "\n",
    "  pixel_size_x = (max_x - min_x) / width\n",
    "  pixel_size_y = (max_y - min_y) / height\n",
    "\n",
    "  x, y = cuda.grid(2) # x = blockIdx.x * blockDim.x + threadIdx.x\n",
    "  if x < width and y < height:\n",
    "      real = min_x + x * pixel_size_x\n",
    "      imag = min_y + y * pixel_size_y\n",
    "      color = mandel_gpu(real, imag, iters)\n",
    "      image[y,x] = color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA58zskpSt46"
   },
   "source": [
    "Finally let's look at mandel. We want it to be a function, which is itself called from another GPU function create_fractal. To specify that it's a device function, use @cuda.jit(device=True) - which is similar to \\_\\_device\\_\\_ in CUDA C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMQ0jl5WTPUO"
   },
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def mandel_gpu(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y8rZ9sGpRbP"
   },
   "source": [
    "Putting it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJRgJ1o7pTJp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import imshow, show\n",
    "from timeit import default_timer as timer\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def mandel_gpu(x, y, max_iters):\n",
    "  \"\"\"\n",
    "    Given the real and imaginary parts of a complex number,\n",
    "    determine if it is a candidate for membership in the Mandelbrot\n",
    "    set given a fixed number of iterations.\n",
    "  \"\"\"\n",
    "  c = complex(x, y)\n",
    "  z = 0.0j\n",
    "  for i in range(max_iters):\n",
    "    z = z*z + c\n",
    "    if (z.real*z.real + z.imag*z.imag) >= 4:\n",
    "      return i\n",
    "\n",
    "  return max_iters\n",
    "\n",
    "@cuda.jit\n",
    "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n",
    "  height = image.shape[0]\n",
    "  width = image.shape[1]\n",
    "\n",
    "  pixel_size_x = (max_x - min_x) / width\n",
    "  pixel_size_y = (max_y - min_y) / height\n",
    "\n",
    "  x, y = cuda.grid(2) # x = blockIdx.x * blockDim.x + threadIdx.x\n",
    "  if x < width and y < height:\n",
    "      real = min_x + x * pixel_size_x\n",
    "      imag = min_y + y * pixel_size_y\n",
    "      color = mandel_gpu(real, imag, iters)\n",
    "      image[y,x] = color\n",
    "\n",
    "image = np.zeros((1024, 1536), dtype = np.uint8)\n",
    "\n",
    "#Create grid of 32x32 blocks, one thread per pixel\n",
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "\n",
    "nthreads = 32\n",
    "nblocksy = (height // nthreads) + 1 # = 33\n",
    "nblocksx = (width // nthreads) + 1 # = 49\n",
    "\n",
    "\n",
    "config = (nblocksx, nblocksy), (nthreads, nthreads)\n",
    "\n",
    "\n",
    "start = timer()\n",
    "create_fractal[config](-2.0, 1.0, -1.0, 1.0, image, 20) \n",
    "dt = timer() - start\n",
    "\n",
    "print (\"Mandelbrot created in %f s\" % dt)\n",
    "imshow(image)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkxsdNdaWPN4"
   },
   "source": [
    "<a id='Python Porting Example'></a>\n",
    "# 2. Python Porting Example\n",
    "\n",
    "This is an example of porting functions to use Numba. It is really an example of the porting\n",
    "<em>lifecycle</em> of a Python application. It starts with a serial Python code\n",
    "that has been written to test the idea and to make sure it works and gets correct answers.\n",
    "\n",
    "Then it moves to putting computational intensive portions of the code into functions.\n",
    "\n",
    "Then it uses Numba to compile for the CPU, using the <tt>@jit</tt> decorator. \n",
    "\n",
    "The next step is to switch to the <tt>@vectorize</tt> decorator, targeting a single core\n",
    "(target is <tt>cpu</tt>), then all of the CPU cores (target is <tt>parallel</tt>), and\n",
    "finally to NVIDIA GPUs (target is <tt>cuda</tt>).\n",
    "\n",
    "The last step is to modify the code to use the <tt>@cuda.jit</tt> decorator. This\n",
    "requires some code changes.\n",
    "\n",
    "The new example code is a very, very simplified version of the start of a Molecular\n",
    "Dynamic (MD) mini-app. It focuses on loops that are initialized (the values are arbitrary\n",
    "and don't mean anything). The code an obviously be made much simpler and faster, but that is\n",
    "not the point. The point is to show you how start with a code that uses loops and \"port\"\n",
    "it to use Numba and GPUs. Several steps will be used in this porting process. Hopefully it\n",
    "gives you some \"feel\" in porting your applications to use Numba.\n",
    "                                                    \n",
    "Let's start with the serial Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fW6sOaIbWPN5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "# main loop\n",
    "start_time = perf_counter( )\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "for j in range(0, p_num):\n",
    "    for i in range(0, d_num):\n",
    "        pos[i,j] = 6.5\n",
    "    # end for i\n",
    "    for i in range(0, d_num):\n",
    "        accel[i,j] = 4.2*pos[i,j]\n",
    "    # end for\n",
    "# end for j\n",
    "\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7C5KRIBWPN9"
   },
   "source": [
    "<a id='Non-Numba Python Code - Function'></a>\n",
    "## 2.1 Non-Numba Python Code\n",
    "\n",
    "The next code creates a function for the computationally intense part of the code (the loop). \n",
    "This gets the code ready for Numba (Remember that Numba compiles functions, not entire codes).\n",
    "Notice that the two arrays are created in the function and are returned to the calling function\n",
    "(both arrays are returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrYS77BRWPN9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "def init(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    return pos, accel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel = init(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olrwFcLoWPOB"
   },
   "source": [
    "Always be sure to check that you answers are correct after you take another\n",
    "step in porting it. Checking answers for this simple code is fairly easy\n",
    "and can be done by simply printing the arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr65W2oMWPOC"
   },
   "source": [
    "<a id='Python Porting Example jit decorator single'></a>\n",
    "## 2.2 <tt>@jit</tt> Decorator Targeting a Single Core on the CPU\n",
    "\n",
    "Next, let us use the <tt>@jit</tt> decorator to compile the function. The first one uses the\n",
    "default target which is a single core.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaBJnKliWPOC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def init(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    return pos, accel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel = init(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqJpRkRfWPOF"
   },
   "source": [
    "<a id='Python Porting Example jit decorator parallel'></a>\n",
    "## 2.3 <tt>@jit</tt> Decorator Targeting Multi-Core (parallel=True)\n",
    "\n",
    "This code simply compiles for multi-core and also disables Python fallback.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyZygsK-WPOF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import clock\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def init(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    return pos, accel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel = init(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1mLiJt_WPOI"
   },
   "source": [
    "<a id='Python Porting Example vectorize single'></a>\n",
    "## 2.4 <tt>@vectorize</tt> Decorator for a Single Core (default)\n",
    "\n",
    "This example rewrites the code to use the <tt>@vectorize</tt> decorator. Remember that this\n",
    "requires the code to be executed element-by-element. It really also requires that\n",
    "each function return one result. This will require some code changes to split the\n",
    "two loops so that each has its own function.\n",
    "\n",
    "Recall that we write the function as if it were a scalar. Numba takes care of\n",
    "all of the details of creating the ufunc based on the code.\n",
    "\n",
    "Another thing to note is that we have to create the arrays in the main routine. They\n",
    "can no longer be created in the functions. The <tt>@vectorize</tt> decorator only wants\n",
    "very simple element-by-element code. The precludes creating the arrays in the functions.\n",
    "\n",
    "Pay close attention to the type signature(s) for the decorator and how the functions\n",
    "are called. It is a bit counter intuitive. \n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ek59G7ZWPOJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'])\n",
    "def set_pos(pos):\n",
    "    return 6.5\n",
    "# end def\n",
    "\n",
    "@vectorize(['float32(float32, float32)'])\n",
    "def set_accel(pos, accel):\n",
    "    return 4.2*pos\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos = set_pos(pos)\n",
    "accel = set_accel(pos,accel)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmB4Ug_RWPOL"
   },
   "source": [
    "<a id='Python Porting Example vectorize parallel'></a>\n",
    "## 2.5 <tt>@vectorize</tt> Decorator for Multi-Core\n",
    "\n",
    "This example simply changes the target for the <tt>@vectorize</tt> decorator to multi-core (parallel).\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_QyXJSFWPOM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'], target='parallel')\n",
    "def set_pos(pos):\n",
    "    return 6.5\n",
    "# end def\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='parallel')\n",
    "def set_accel(pos, accel):\n",
    "    return 4.2*pos\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos = set_pos(pos)\n",
    "accel = set_accel(pos,accel)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDreYytQWPOQ"
   },
   "source": [
    "<a id='Python Porting Example vectorize cuda'></a>\n",
    "## 2.6 <tt>@vectorize</tt> Decorator - CUDA Target\n",
    "\n",
    "This next example also simply changes the target, but this time it is for NVIDIA GPUs.\n",
    "THis is a benefit of being able to write your function code as scalars and using Numba\n",
    "to vectorize it. You can just change the target to get either a single CPU core,\n",
    "multiple CPU cores, or an NVIDIA GPU.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4p4LSp3WPOR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def set_pos(pos):\n",
    "    return 6.5\n",
    "# end def\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def set_accel(pos, accel):\n",
    "    return 4.2*pos\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos = set_pos(pos)\n",
    "accel = set_accel(pos,accel)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WWcVapqWPOk"
   },
   "source": [
    "<a id='CUPY'></a>\n",
    "## 3. CuPy\n",
    "\n",
    "As discussed in the slides, CuPy is roughly a GPU equivalent for NumPy. It covers virtually\n",
    "all of the NumPy functions but runs them on a GPU. In addition, CuPy is also starting to port\n",
    "some SciPy routine to the GPU in a new library named <tt>cupyx.scipy</tt>.\n",
    "\n",
    "The following examples illustrate how CuPy can be used in your Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIYt1F66WPOk"
   },
   "source": [
    "<a id='CUPY SVD Data on GPU'></a>\n",
    "## 3.1 SVD Example, Leaving the Data on the GPU\n",
    "\n",
    "This example performs a Singular Value Decomposition (SVD) on a random matrix that is\n",
    "created on the GPU. Note that the results of the SVD, the <tt>u</tt>, <tt>v</tt>, and\n",
    "<tt>s</tt> matrices, are available on the GPU after the computation but in this example,\n",
    "they are not copied back to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97sXphEZWPOl",
    "outputId": "b65f6f14-93fb-49f6-cd9d-5ae1f18c05ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Elapsed wall clock time for numpy = 0.976656 seconds.\n",
      "\n",
      "\n",
      "    Elapsed wall clock time for cupy = 0.399943 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxZJ0-HuWPOo"
   },
   "source": [
    "<a id='CUPY SVD Data back to host'></a>\n",
    "## 3.2 SVD Example - Copy Data Back to the Host\n",
    "\n",
    "This example is the same as the previous, but the <tt>u</tt> matrix is copied\n",
    "back to the host using the <tt>asnumpy</tt> method. The \"type\" of the <tt>u</tt>\n",
    "matrix on the GPU and the <tt>u</tt> matrix on the CPU are both printed so you\n",
    "can tell that one is on the device (GPU) and one is on the host (CPU). It also\n",
    "checks the difference between the reconstructed <tt>A</tt> matrix from the SVD\n",
    "components, versus the original <tt>A</tt> matrix.\n",
    "\n",
    "Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb476QvyWPOo"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "A_cpu = np.random.uniform(low=-1., high=1., size=(64, 64)).astype(np.float32)\n",
    "A_gpu = cp.asarray(A_cpu)\n",
    "\n",
    "u_gpu, s_gpu, v_gpu = cp.linalg.svd(A_gpu)\n",
    "print(\"type(u_gpu) = \",type(u_gpu) )\n",
    "\n",
    "u_cpu = cp.asnumpy(u_gpu)\n",
    "print(\"type(u_cpu) = \",type(u_cpu) )\n",
    "\n",
    "# ----- Check answer -----\n",
    "v_cpu = cp.asnumpy(v_gpu)\n",
    "s_cpu = cp.asnumpy(s_gpu)\n",
    "\n",
    "s_cpu_test = np.diag(s_cpu)\n",
    "result = np.allclose(A_cpu, np.dot(u_cpu, np.dot(s_cpu_test, v_cpu)), atol=1e-05)\n",
    "print(\"Check result = \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXqkT63ZWPOr"
   },
   "source": [
    "<a id='CUPY Matrix Mult Data on GPU'></a>\n",
    "## 3.3 Matrix Multiplication Example - Create Data on GPU\n",
    "\n",
    "This example creates random data on the CPU or GPU and then performs the matrix\n",
    "multiplication. Note that the result of the multiplication, the <tt>C</tt> matrix,\n",
    "is available on the GPU if you need it.\n",
    "\n",
    "Notice the similarity of the two parts of the code (numpy and cupy).\n",
    "They are virtually identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRMSiXxDWPOr",
    "outputId": "6f137923-d6fe-4c11-a263-6b85f88b3360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Elapsed wall clock time for numpy = 2.18571 seconds.\n",
      "\n",
      "\n",
      "    Elapsed wall clock time for cupy = 1.89381 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "size = 8000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C = np.matmul(A,B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A\n",
    "del B\n",
    "del C\n",
    "\n",
    "\n",
    "A = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "\n",
    "start_time = perf_counter( )\n",
    "#A = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "#B = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C = cp.matmul(A,B)\n",
    "cp.cuda.Device(0).synchronize()\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A\n",
    "del B\n",
    "del C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGAqrOjGWPOu"
   },
   "source": [
    "<a id='CUPY Matrix Mult Data on CPU'></a>\n",
    "## 3.4 Matrix Multiplication Example - Copy Data from Host to GPU\n",
    "\n",
    "This example creates the data on the CPU and then copies it to\n",
    "the GPU. Matrix Multiplication on the CPU and GPU is timed. The\n",
    "GPU timing includes the time for the data movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6NFpJRvWPOv"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "size = 8000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C = np.matmul(A,B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_gpu = cp.asarray(A)\n",
    "B_gpu = cp.asarray(B)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "C_cpu = cp.asnumpy(C_gpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnS7dcXgck_v"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3zrk5sIXHvb"
   },
   "source": [
    "Setup for colab environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Je_lXPhjWo3h"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "MINICONDA_PREFIX=/usr/local\n",
    "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
    "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
    "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjqEnWEXXgqV"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda install --channel defaults conda python=3.6 --yes\n",
    "conda update --channel defaults --all --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIp4PL4MXhX7"
   },
   "outputs": [],
   "source": [
    "!conda --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wz1FK35bX5BQ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "_ = (sys.path.append(\"/usr/local/lib/python3.6/site-packages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXWl51APX571"
   },
   "outputs": [],
   "source": [
    "!conda install --channel conda-forge cupy numba dask tbb --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thh3LyXELtlR"
   },
   "outputs": [],
   "source": [
    "!conda list -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oksWWcn-Mfc_"
   },
   "outputs": [],
   "source": [
    "!conda --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhgtgZhabihx"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uoq91v_BbCDo"
   },
   "outputs": [],
   "source": [
    "!export NUMBA_THREADING_LAYER='omp' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCqut7KiwWCx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "l7C5KRIBWPN9",
    "Mr65W2oMWPOC",
    "lqJpRkRfWPOF",
    "g1mLiJt_WPOI",
    "CmB4Ug_RWPOL",
    "KDreYytQWPOQ",
    "qHK_yobFWPOU",
    "qggiSswOWPOb",
    "W3nI3kcUWPOe",
    "mL-sRTLeWPOh",
    "3WWcVapqWPOk",
    "kIYt1F66WPOk",
    "UxZJ0-HuWPOo",
    "wXqkT63ZWPOr",
    "bGAqrOjGWPOu",
    "F2LzOOqLWPOy",
    "i_paYNIyWPO2",
    "zGwM0eyzWPO5",
    "8ykz2yFoWPO8",
    "Q3z8EEosWPPA",
    "_sRnkE4pWPPF"
   ],
   "name": "pygpu-final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
